Chap 6 Working with Different Types of Data
-------------------------------------------

This chapter covers building expressions, which are the bread and butter of Spark’s structured operations.
We also review working with a variety of different kinds of data, including the following:
Booleans
Numbers
Strings
Dates and timestamps
Handling null
Complex types
User-defined functions

-------------------------------------------
Converting to Spark Types
-------------------------------------------
the lit function. This function converts a
type in another language to its correspnding Spark representation.

// in Scala
import org.apache.spark.sql.functions.lit
df.select(lit(5), lit("five"), lit(5.0))

There’s no equivalent function necessary in SQL, so we can use the values directly:
-- in SQL
SELECT 5, "five", 5.0

-------------------------------------------
Working with Booleans
-------------------------------------------
Boolean statements consist of four elements: and, or, true, and false. We use these
simple structures to build logical statements that evaluate to either true or false.

// in Scala
import org.apache.spark.sql.functions.col
df.where(col("InvoiceNo").equalTo(536365))
.select("InvoiceNo", "Description")
.show(5, false)

WARNING
Scala has some particular semantics regarding the use of == and ===. In Spark, if you want to filter by
equality you should use === (equal) or =!= (not equal). You can also use the not function and the
equalTo method.

Another option—and probably the cleanest—is to specify the predicate as an expression in a
string. This is valid for Python or Scala. Note that this also gives you access to another way of
expressing “does not equal”:

df.where("InvoiceNo = 536365").show(5, false)

df.where("InvoiceNo <> 536365").show(5, false)

Boolean statements are expressed serially (one after the other),
Spark will flatten all of these filters into one statement and perform the filter at the same time

// in Scala
val priceFilter = col("UnitPrice") > 600
val descripFilter = col("Description").contains("POSTAGE")
df.where(col("StockCode").isin("DOT")).where(priceFilter.or(descripFilter))
.show()

-------------------------------------------
Working with Numbers
-------------------------------------------
pow:
// in Scala
import org.apache.spark.sql.functions.{expr, pow}
val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
df.select(expr("CustomerId"), fabricatedQuantity.alias("realQuantity")).show(2)

Notice that we were able to multiply our columns together because they were both numerical.
Naturally we can add and subtract as necessary, as well. In fact, we can do all of this as a SQL
expression, as well:
// in Scala
df.selectExpr(
"CustomerId",
"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity").show(2)
# in Python
df.selectExpr(
"CustomerId",
"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity").show(2)
-- in SQL
SELECT customerId, (POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity
FROM dfTable

round:
// in Scala
import org.apache.spark.sql.functions.{round, bround}
df.select(round(col("UnitPrice"), 1).alias("rounded"), col("UnitPrice")).show(5)

By default, the round function rounds up if you’re exactly in between two numbers. You can
round down by using the bround:
// in Scala
import org.apache.spark.sql.functions.lit
df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)

Another numerical task is to compute the correlation of two columns. For example, we can see
the Pearson correlation coefficient for two columns to see if cheaper things are typically bought
in greater quantities. We can do this through a function as well as through the DataFrame
statistic methods:
// in Scala
import org.apache.spark.sql.functions.{corr}
df.stat.corr("Quantity", "UnitPrice")
df.select(corr("Quantity", "UnitPrice")).show()

Another common task is to compute summary statistics for a column or set of columns. We can
use the describe method to achieve exactly this. This will take all numeric columns and
calculate the count, mean, standard deviation, min, and max. You should use this primarily for
viewing in the console because the schema might change in the future:
// in Scala
df.describe().show()

As a last note, we can also add a unique ID to each row by using the function
monotonically_increasing_id. This function generates a unique value for each row, starting
with 0:// in Scala
import org.apache.spark.sql.functions.monotonically_increasing_id
df.select(monotonically_increasing_id()).show(2)

-------------------------------------------
Working with Strings
-------------------------------------------
The initcap function will
capitalize every word in a given string when that word is separated from another by a space.
// in Scala
import org.apache.spark.sql.functions.{initcap}
df.select(initcap(col("Description"))).show(2, false)

As just mentioned, you can cast strings in uppercase and lowercase, as well:
// in Scalaimport org.apache.spark.sql.functions.{lower, upper}
df.select(col("Description"),
lower(col("Description")),
upper(lower(col("Description")))).show(2)

Another trivial task is adding or removing spaces around a string. You can do this by using lpad,
ltrim, rpad and rtrim, trim
// in Scala
import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}
df.select(
ltrim(lit("  HELLO   ")).as("ltrim"),
rtrim(lit("  HELLO   ")).as("rtrim"),
trim(lit("   HELLO   ")).as("trim"),
lpad(lit("HELLO"), 3, " ").as("lp"),
rpad(lit("HELLO"), 10, " ").as("rp")).show(2)

-------------------------------------------
Regular Expressions
-------------------------------------------
There are two key functions in Spark that you’ll need in
order to perform regular expression tasks: regexp_extract and regexp_replace. These
functions extract values and replace values, respectively.

// in Scala
import org.apache.spark.sql.functions.regexp_replace
val simpleColors = Seq("black", "white", "red", "green", "blue")
val regexString = simpleColors.map(_.toUpperCase).mkString("|")
// the | signifies `OR` in regular expression syntax
df.select(
regexp_replace(col("Description"), regexString, "COLOR").alias("color_clean"),
col("Description")).show(2)

-------------------------------------------
Working with Dates and Timestamps
-------------------------------------------
Spark does its best to keep things simple by focusing explicitly on two kinds of time-related information.
There are dates, which focus exclusively on calendar dates, and timestamps, which include both date
and time information.

import org.apache.spark.sql.functions.{current_date, current_timestamp}
val dateDF = spark.range(10)
.withColumn("today", current_date())
.withColumn("now", current_timestamp())
dateDF.createOrReplaceTempView("dateTable")

Now that we have a simple DataFrame to work with, let’s add and subtract five days from today.
These functions take a column and then the number of days to either add or subtract as the
arguments:
// in Scala
import org.apache.spark.sql.functions.{date_add, date_sub}
dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1)

months_between, that gives you the number of months between two dates:
// in Scala
import org.apache.spark.sql.functions.{datediff, months_between, to_date}
dateDF.withColumn("week_ago", date_sub(col("today"), 7)).select(datediff(col("week_ago"), col("today"))).show(1)
dateDF.select(
to_date(lit("2016-01-01")).alias("start"),
to_date(lit("2017-05-22")).alias("end"))
.select(months_between(col("start"), col("end"))).show(1)

The to_date function allows
you to convert a string to a date, optionally with a specified format. We specify our format in the
Java SimpleDateFormat which will be important to reference if you use this function:
// in Scala
import org.apache.spark.sql.functions.{to_date, lit}
spark.range(5).withColumn("date", lit("2017-01-01"))
.select(to_date(col("date"))).show(1)

Spark will not throw an error if it cannot parse the date; rather, it will just return null. This can
be a bit tricky in larger pipelines because you might be expecting your data in one format and
getting it in another.

WARNING
Implicit type casting is an easy way to shoot yourself in the foot, especially when dealing with null
values or dates in different timezones or formats. We recommend that you parse them explicitly
instead of relying on implicit conversions.

-------------------------------------------
Working with Nulls in Data
-------------------------------------------
WARNING
Nulls are a challenging part of all programming, and Spark is no exception. In our opinion, being
explicit is always better than being implicit when handling null values. For instance, in this part of the
book, we saw how we can define columns as having null types. However, this comes with a catch.
When we declare a column as not having a null time, that is not actually enforced. To reiterate, when
you define a schema in which all columns are declared to not have null values, Spark will not enforce
that and will happily let null values into that column. The nullable signal is simply to help Spark SQL
optimize for handling that column. If you have null values in columns that should not have null values,
you can get an incorrect result or see strange exceptions that can be difficult to debug.

There are two things you can do with null values: you can explicitly drop nulls or you can fill
them with a value (globally or on a per-column basis).

1 Coalesce
-----------
Spark includes a function to allow you to select the first non-null value from a set of columns by
using the coalesce function. In this case, there are no null values, so it simply returns the first
column:
// in Scala
import org.apache.spark.sql.functions.coalesce
df.select(coalesce(col("Description"), col("CustomerId"))).show()

2.ifnull, nullIf, nvl, and nvl2
--------------------------------
ifnull allows
you to select the second value if the first is null, and defaults to the first. Alternatively, you could
use nullif, which returns null if the two values are equal or else returns the second if they are
not. nvl returns the second value if the first is null, but defaults to the first. Finally, nvl2 returns
the second value if the first is not null; otherwise, it will return the last specified value
(else_value in the following example)

-- in SQL
SELECT
ifnull(null, 'return_value'),
nullif('value', 'value'),nvl(null, 'return_value'),
nvl2('not_null', 'return_value', "else_value")
FROM dfTable LIMIT 1
+------------+----+------------+------------+
| a         |   b|            c|            d|
+------------+----+------------+------------+
|return_value|null|return_value|return_value|
+------------+----+------------+------------+

3.drop
------
The simplest function is drop, which removes rows that contain nulls. The default is to drop any
row in which any value is null:

df.na.drop()
df.na.drop("any")
df.na.drop("all")

We can also apply this to certain sets of columns by passing in an array of columns:
// in Scala
df.na.drop("all", Seq("StockCode", "InvoiceNo"))

In SQL, we have to do this column by column:
-- in SQL
SELECT * FROM dfTable WHERE Description IS NOT NULL

4.fill
------
Using the fill function, you can fill one or more columns with a set of values. This can be done
by specifying a map—that is a particular value and a set of columns.
For example, to fill all null values in columns of type String, you might specify the following:

df.na.fill("All Null values become this string")

5.replace
---------
Probably the most common use case is
to replace all values in a certain column according to their current value. The only requirement is
that this value be the same type as the original value:
// in Scala
df.na.replace("Description", Map("" -> "UNKNOWN"))

6.Ordering
----------
you can use asc_nulls_first, desc_nulls_first,
asc_nulls_last, or desc_nulls_last to specify where you would like your null values to
appear in an ordered DataFrame.

-------------------------------------------
Working with Complex Types
-------------------------------------------
There are three kinds of complex types: structs, arrays, and maps.

1.Structs
---------
You can think of structs as DataFrames within DataFrames. A worked example will illustrate
this more clearly. We can create a struct by wrapping a set of columns in parenthesis in a query:

df.selectExpr("(Description, InvoiceNo) as complex", "*")
df.selectExpr("struct(Description, InvoiceNo) as complex", "*")

We now have a DataFrame with a column complex. We can query it just as we might another
DataFrame, the only difference is that we use a dot syntax to do so, or the column method
getField:
complexDF.select("complex.Description")
complexDF.select(col("complex").getField("Description"))

complexDF.select("complex.*")

2.Arrays
--------
With our current data, our objective is to take
every single word in our Description column and convert that into a row in our DataFrame.
The first task is to turn our Description column into a complex type, an array.

split
-----
df.select(split(col("Description"), " ")).show(2)

size
----
// in Scala
import org.apache.spark.sql.functions.size
df.select(size(split(col("Description"), " "))).show(2) // shows 5 and 3

array_contains
--------------
We can also see whether this array contains a value:
// in Scala
import org.apache.spark.sql.functions.array_contains
df.select(array_contains(split(col("Description"), " "), "WHITE")).show(2)

explode
-------
The explode function takes a column that consists of arrays and creates one row (with the rest of
the values duplicated) per value in the array.

// in Scala
import org.apache.spark.sql.functions.{split, explode}
df.withColumn("splitted", split(col("Description"), " "))
.withColumn("exploded", explode(col("splitted")))
.select("Description", "InvoiceNo", "exploded").show(2)

3.Maps
------
Maps are created by using the map function and key-value pairs of columns. You then can select
them just like you might select from an array:

// in Scala
import org.apache.spark.sql.functions.map
df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).show(2)

You can query them by using the proper key. A missing key returns null:

// in Scala
df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map"))
.selectExpr("complex_map['WHITE METAL LANTERN']").show(2)

You can also explode map types, which will turn them into columns:
// in Scala
df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map"))
.selectExpr("explode(complex_map)").show(2)

-------------------------------------------
Working with JSON
-------------------------------------------
Spark has some unique support for working with JSON data. You can operate directly on strings
of JSON in Spark and parse from JSON or extract JSON objects. Let’s begin by creating a JSON
column:

// in Scala
val jsonDF = spark.range(1).selectExpr("""
'{"myJSONKey" : {"myJSONValue" : [1, 2, 3]}}' as jsonString""")

You can also turn a StructType into a JSON string by using the to_json function:

// in Scala
import org.apache.spark.sql.functions.to_json
df.selectExpr("(InvoiceNo, Description) as myStruct")
.select(to_json(col("myStruct")))

-------------------------------------------
User-Defined Functions
-------------------------------------------
One of the most powerful things that you can do in Spark is define your own functions. These
user-defined functions (UDFs) make it possible for you to write your own custom
transformations using Python or Scala and even use external libraries. UDFs can take and return
one or more columns as input. Spark UDFs are incredibly powerful because you can write them
in several different programming languages; you do not need to create them in an esoteric format
or domain-specific language. They’re just functions that operate on the data, record by record.
By default, these functions are registered as temporary functions to be used in that specific
SparkSession or Context.

Although you can write UDFs in Scala, Python, or Java, there are performance considerations
that you should be aware of.

Now that we’ve created these functions and tested them, we need to register them with Spark so
that we can use them on all of our worker machines. Spark will serialize the function on the
driver and transfer it over the network to all executor processes. This happens regardless of
language.

WARNING
Starting this Python process is expensive, but the real cost is in serializing the data to Python. This is
costly for two reasons: it is an expensive computation, but also, after the data enters Python, Spark
cannot manage the memory of the worker. This means that you could potentially cause a worker to fail
if it becomes resource constrained (because both the JVM and Python are competing for memory on
the same machine). We recommend that you write your UDFs in Scala or Java—the small amount of
time it should take you to write the function in Scala will always yield significant speed ups, and on
top of that, you can still use the function from Python!


