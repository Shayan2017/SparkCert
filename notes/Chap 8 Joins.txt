Chapter 8. Joins
#############################################
This chapter covers not just what joins exist in Spark and how to use them, but some
of the basic internals so that you can think about how Spark actually goes about executing the
join on the cluster. This basic knowledge can help you avoid running out of memory and tackle
problems that you could not solve before.

=============================================
Join Expressions
=============================================
A join brings together two sets of data, the left and the right, by comparing the value of one or
more keys of the left and right and evaluating the result of a join expression that determines
whether Spark should bring together the left set of data with the right set of data. The most
common join expression, an equi-join, compares whether the specified keys in your left and
right datasets are equal. If they are equal, Spark will combine the left and right datasets.

=============================================
Join Types
=============================================
Whereas the join expression determines whether two rows should join, the join type determines
what should be in the result set. There are a variety of different join types available in Spark for
you to use

---------------------------------------------
1.Inner Joins
---------------------------------------------
Inner joins evaluate the keys in both of the DataFrames or tables and include (and join together)
only the rows that evaluate to true.

Keys that do not exist in both DataFrames will not show in the resulting DataFrame.

person.join(graduateProgram, joinExpression).show()

var joinType = "inner"
person.join(graduateProgram, joinExpression, joinType).show()

---------------------------------------------
2.Outer Joins
---------------------------------------------
Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins
together) the rows that evaluate to true or false. If there is no equivalent row in either the left or
right DataFrame, Spark will insert null:

joinType = "outer"
person.join(graduateProgram, joinExpression, joinType).show()

---------------------------------------------
3.Left Outer Joins
---------------------------------------------
Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from
the left DataFrame as well as any rows in the right DataFrame that have a match in the left
DataFrame. If there is no equivalent row in the right DataFrame, Spark will insert null:

joinType = "left_outer"
graduateProgram.join(person, joinExpression, joinType).show()

---------------------------------------------
4.Right Outer Joins
---------------------------------------------
Right outer joins evaluate the keys in both of the DataFrames or tables and includes all rows
from the right DataFrame as well as any rows in the left DataFrame that have a match in the right
DataFrame. If there is no equivalent row in the left DataFrame, Spark will insert null:

joinType = "right_outer"
person.join(graduateProgram, joinExpression, joinType).show()

---------------------------------------------
5.Left Semi Joins
---------------------------------------------
Semi joins are a bit of a departure from the other joins. They do not actually include any values
from the right DataFrame. They only compare values to see if the value exists in the second
DataFrame. If the value does exist, those rows will be kept in the result, even if there are
duplicate keys in the left DataFrame. Think of left semi joins as filters on a DataFrame, as
opposed to the function of a conventional join:

joinType = "left_semi"
graduateProgram.join(person, joinExpression, joinType).show()

---------------------------------------------
6.Left Anti Joins
---------------------------------------------
Left anti joins are the opposite of left semi joins. Like left semi joins, they do not actually
include any values from the right DataFrame. They only compare values to see if the value exists
in the second DataFrame. However, rather than keeping the values that exist in the second
DataFrame, they keep only the values that do not have a corresponding key in the second
DataFrame.

---------------------------------------------
7.Natural Joins
---------------------------------------------
Natural joins make implicit guesses at the columns on which you would like to join. It finds
matching columns and returns the results. Left, right, and outer natural joins are all supported.

WARNING
Implicit is always dangerous! The following query will give us incorrect results because the two
DataFrames/tables share a column name (id), but it means different things in the datasets. You should
always use this join with caution.

-- in SQL
SELECT * FROM graduateProgram NATURAL JOIN person

---------------------------------------------
8.Cross (Cartesian) Joins
---------------------------------------------
The last of our joins are cross-joins or cartesian products. Cross-joins in simplest terms are inner
joins that do not specify a predicate. Cross joins will join every single row in the left DataFrame
to ever single row in the right DataFrame. This will cause an absolute explosion in the number of
rows contained in the resulting DataFrame. If you have 1,000 rows in each DataFrame, the cross-
join of these will result in 1,000,000 (1,000 x 1,000) rows. For this reason, you must very
explicitly state that you want a cross-join by using the cross join keyword:

*******joinType = "cross"
graduateProgram.join(person, joinExpression, joinType).show()

WARNING
You should use cross-joins only if you are absolutely, 100 percent sure that this is the join you need.
There is a reason why you need to be explicit when defining a cross-join in Spark. They’re dangerous!
Advanced users can set the session-level configuration spark.sql.crossJoin.enable to true in
order to allow cross-joins without warnings or without Spark trying to perform another join for you.

=============================================
Challenges When Using Joins
=============================================

---------------------------------------------
Joins on Complex Types
---------------------------------------------
Even though this might seem like a challenge, it’s actually not. Any expression is a valid join
expression, assuming that it returns a Boolean

import org.apache.spark.sql.functions.expr
person.withColumnRenamed("id", "personId")
.join(sparkStatus, expr("array_contains(spark_status, id)")).show()

---------------------------------------------
Handling Duplicate Column Names
---------------------------------------------
One of the tricky things that come up in joins is dealing with duplicate column names in your
results DataFrame. In a DataFrame, each column has a unique ID within Spark’s SQL Engine,
Catalyst. This unique ID is purely internal and not something that you can directly reference.
This makes it quite difficult to refer to a specific column when you have a DataFrame with
duplicate column names.

This can occur in two distinct situations:
The join expression that you specify does not remove one key from one of the input
DataFrames and the keys have the same column name
Two columns on which you are not performing the join have the same name

Approach 1: Different join expression
-------------------------------------
When you have two keys that have the same name, probably the easiest fix is to change the join
expression from a Boolean expression to a string or sequence. This automatically removes one of
the columns for you during the join:
person.join(gradProgramDupe,"graduate_program").select("graduate_program").show()

Approach 2: Dropping the column after the join
----------------------------------------------
person.join(gradProgramDupe, joinExpr).drop(person.col("graduate_program"))
.select("graduate_program").show()

Approach 3: Renaming a column before the join
---------------------------------------------
We can avoid this issue altogether if we rename one of our columns before the join

=============================================
How Spark Performs Joins
=============================================
To understand how Spark performs joins, you need to understand the two core resources at play:
the node-to-node communication strategy and per node computation strategy. These internals are
likely irrelevant to your business problem. However, comprehending how Spark performs joins
can mean the difference between a job that completes quickly and one that never completes at
all.

---------------------------------------------
Communication Strategies
---------------------------------------------
Spark approaches cluster communication in two different ways during joins. It either incurs a
shuffle join, which results in an all-to-all communication or a broadcast join.

Big table–to–big table
----------------------
When you join a big table to another big table, you end up with a shuffle join

In a shuffle join, every node talks to every other node and they share data according to which
node has a certain key or set of keys (on which you are joining). These joins are expensive
because the network can become congested with traffic, especially if your data is not partitioned
well.

Big table–to–small table
------------------------
When the table is small enough to fit into the memory of a single worker node, with some
breathing room of course, we can optimize our join. Although we can use a big table–to–big
table communication strategy, it can often be more efficient to use a broadcast join.

What this means is that we will replicate our small DataFrame onto every worker node in
the cluster (be it located on one machine or many)

