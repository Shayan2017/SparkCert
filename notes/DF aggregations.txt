Aggregation Functions

-count
// in Scala
import org.apache.spark.sql.functions.count
df.select(count("StockCode")).show()

df.select("StockCode").count

-countDistinct
// in Scala
import org.apache.spark.sql.functions.countDistinct
df.select(countDistinct("StockCode")).show() // 4070

-approx_count_distinct
// in Scala
import org.apache.spark.sql.functions.approx_count_distinct
df.select(approx_count_distinct("StockCode", 0.1)).show() // 3364

-first and last
// in Scala
import org.apache.spark.sql.functions.{first, last}
df.select(first("StockCode"), last("StockCode")).show()

-min and max
// in Scala
import org.apache.spark.sql.functions.{min, max}
df.select(min("Quantity"), max("Quantity")).show()

-sum
// in Scala
import org.apache.spark.sql.functions.sum
df.select(sum("Quantity")).show() // 5176450

-sumDistinct
df.select(sumDistinct("Quantity")).show() // 29310

-avg
// in Scala
import org.apache.spark.sql.functions.{sum, count, avg, expr}
df.select(
    count("Quantity").alias("total_transactions"),
    sum("Quantity").alias("total_purchases"),
    avg("Quantity").alias("avg_purchases"),
    expr("mean(Quantity)").alias("mean_purchases"))
    .selectExpr(
        "total_purchases/total_transactions",
        "avg_purchases",
        "mean_purchases")
.show()



Grouping

df.groupBy("InvoiceNo", "CustomerId").count().show()
-- in SQL
SELECT count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId


-Grouping with Expressions
df.groupBy("InvoiceNo").agg(
    count("Quantity").alias("quan"),
    expr("count(Quantity)"))
.show()

-Grouping with Maps
// in Scala
df.groupBy("InvoiceNo").agg("Quantity"->"avg", "Quantity"->"stddev_pop").show()

+---------+------------------+--------------------+
|InvoiceNo| avg(Quantity)|stddev_pop(Quantity)|
+---------+------------------+--------------------+
| 536596| 1.5| 1.1180339887498947




Window Functions

Spark supports three kinds of window functions:
ranking functions,
analytic functions,
and aggregate functions.




Grouping Sets

