Chapter 4. Structured API Overview
----------------------------------
The Structured APIs are a tool for manipulating all sorts of data, 
from unstructured log files to semi-structured CSV files and highly structured Parquet files. 
These APIs refer to three core types of distributed collection APIs:
Datasets
DataFrames
SQL tables and views

Before proceeding, let’s review the fundamental concepts and definitions that we covered in Part I.
Spark is a distributed programming model in which the user specifies transformations. Multiple
transformations build up a directed acyclic graph of instructions. An action begins the process of
executing that graph of instructions, as a single job, by breaking it down into stages and tasks to
execute across the cluster. The logical structures that we manipulate with transformations and actions
are DataFrames and Datasets. To create a new DataFrame or Dataset, you call a transformation. To
start computation or convert to native language types, you call an action.

DataFrames and Datasets
-----------------------
DataFrames and Datasets are (distributed) table-like collections with well-defined rows and
columns. Each column must have the same number of rows as all the other columns (although
you can use null to specify the absence of a value) and each column has type information that
must be consistent for every row in the collection. 

To Spark, DataFrames and Datasets represent
immutable, lazily evaluated plans that specify what operations to apply to data residing at a
location to generate some output. When we perform an action on a DataFrame, we instruct Spark
to perform the actual transformations and return the result. These represent plans of how to
manipulate rows and columns to compute the user’s desired result.

Schemas
-------
A schema defines the column names and types of a DataFrame. You can define schemas
manually or read a schema from a data source (often called schema on read). Schemas consist of
types, meaning that you need a way of specifying what lies where.

Overview of Structured Spark Types
----------------------------------
Spark is effectively a programming language of its own. Internally, Spark uses an engine called
Catalyst that maintains its own type information through the planning and processing of work.

DataFrames Versus Datasets
--------------------------
In essence, within the Structured APIs, there are two more APIs, the “untyped” DataFrames and
the “typed” Datasets. To say that DataFrames are untyped is aslightly inaccurate; they have
types, but Spark maintains them completely and only checks whether those types line up to those
specified in the schema at runtime. 

Datasets, on the other hand, check whether types conform to
the specification at compile time. Datasets are only available to Java Virtual Machine (JVM)–
based languages (Scala and Java) and we specify types with case classes or Java beans.

To Spark (in Scala), DataFrames are simply Datasets of Type Row. 
The “Row” type is Spark’s internal representation of its optimized in-memory format for computation. 
This format makes for highly specialized and efficient computation because rather than using JVM types, 
which can cause high garbage-collection and object instantiation costs, 
Spark can operate on its own internal format without incurring any of those costs. 

To Spark (in Python or R), there is no such thing as a Dataset: everything is a
DataFrame and therefore we always operate on that optimized format.

Columns
-------
Columns represent a simple type like an integer or string, a complex type like an array or map, or
a null value. Spark tracks all of this type information for you and offers a variety of ways, with
which you can transform columns. Columns are discussed extensively in Chapter 5, but for the
most part you can think about Spark Column types as columns in a table.

Rows
----
A row is nothing more than a record of data. Each record in a DataFrame must be of type Row, as
we can see when we collect the following DataFrames. We can create these rows manually from
SQL, from Resilient Distributed Datasets (RDDs), from data sources, or manually from scratch.

Spark Types
-----------
Table 4-2. Scala type reference
Data type | Value type in Scala | API to access or create a data type
ByteType Byte ByteType
ShortType Short ShortType
IntegerType Int IntegerType
LongType Long LongType
FloatType Float FloatType
DoubleType Double DoubleType
DecimalType java.math.BigDecimal DecimalType
StringType String StringType
BinaryType Array[Byte] BinaryType
BooleanType Boolean BooleanType
TimestampType java.sql.Timestamp TimestampType
DateType java.sql.Date DateType
ArrayType scala.collection.Seq
ArrayType(elementType, [containsNull]).
Note: The default value of containsNull is
true.
MapType scala.collection.Map
MapType(keyType, valueType,
[valueContainsNull]). Note: The default
value of valueContainsNull is true.
StructType org.apache.spark.sql.Row
StructType(fields). Note: fields is an
Array of StructFields. Also, fields with
the same name are not allowed.
StructField
The value type in Scala of the data type of
this field (for example, Int for a StructField
with the data type IntegerType)
StructField(name, dataType, [nullable]).
Note: The default value of nullable is true.

Overview of Structured API Execution
------------------------------------
1. Write DataFrame/Dataset/SQL Code.
2. If valid code, Spark converts this to a Logical Plan.
3. Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along the way.
4. Spark then executes this Physical Plan (RDD manipulations) on the cluster.

Logical Planning
----------------
User Code = Unresolved Logical Plan
Spark Catalog analysis and generates Resolved Logical Plan
If the above (analyzer can resolve it) is done, the result is passed through 
the Catalyst Optimizer, a collection of rules that attempt to optimize the
logical plan by pushing down predicates or selections.

Physical Planning
-----------------
After successfully creating an optimized logical plan, Spark then begins the physical planning
process. The physical plan, often called a Spark plan, specifies how the logical plan will execute
on the cluster by generating different physical execution strategies and comparing them through
a cost model

